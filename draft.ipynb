{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Annotation\n",
    "from pyannote.pipeline import Optimizer\n",
    "from pyannote.metrics.detection import DetectionErrorRate\n",
    "from pyannote.database import registry, get_protocol, FileFinder\n",
    "from pyannote.audio.tasks import VoiceActivityDetection\n",
    "from pyannote.audio.models.segmentation import PyanNet\n",
    "from pyannote.audio import Inference\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection as VoiceActivityDetectionPipeline\n",
    "from utils import Get_RTTM\n",
    "import pytorch_lightning as pl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'My_datasets.SpeakerDiarization.Detection' found in /mnt/e/Files/Acoustic_Data/Datasets/yaml/My_Databases.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n"
     ]
    }
   ],
   "source": [
    "# Load database and set environment variable\n",
    "registry.load_database(\n",
    "    os.path.join('/mnt/', 'e', 'Files', 'Acoustic_Data', 'Datasets', 'yaml','My_Databases.yml')\n",
    ")\n",
    "os.environ[\"PYANNOTE_DATABASE_CONFIG\"] = os.path.join('/mnt/', 'e', 'Files', 'Acoustic_Data', 'Datasets','yaml', 'My_Databases.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get protocol and initial training file\n",
    "preprocessors = {\"audio\": FileFinder()}\n",
    "cow_audio = get_protocol('My_datasets.SpeakerDiarization.Detection', preprocessors=preprocessors)\n",
    "first_training_file = next(cow_audio.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /mnt/e/Files/Acoustic_Data/Datasets/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol My_datasets.SpeakerDiarization.Detection does not precompute the output of torchaudio.info(): adding a 'torchaudio.info' preprocessor for you to speed up dataloaders. See pyannote.database documentation on how to do that yourself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type             | Params | In sizes      | Out sizes                                 \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "0 | sincnet           | SincNet          | 42.6 K | [1, 1, 16000] | [1, 60, 56]                               \n",
      "1 | lstm              | LSTM             | 589 K  | [1, 56, 60]   | [[1, 56, 256], [[4, 1, 128], [4, 1, 128]]]\n",
      "2 | linear            | ModuleList       | 49.4 K | ?             | ?                                         \n",
      "3 | classifier        | Linear           | 129    | [1, 56, 128]  | [1, 56, 1]                                \n",
      "4 | activation        | Sigmoid          | 0      | [1, 56, 1]    | [1, 56, 1]                                \n",
      "5 | validation_metric | MetricCollection | 0      | ?             | ?                                         \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "681 K     Trainable params\n",
      "0         Non-trainable params\n",
      "681 K     Total params\n",
      "2.728     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6107721833204d0cb05a9b60ebde5768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbf51615b4d4b6a98c716efa61a9e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f35dbc12fc74585b712ccbc06037566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dfd23ba3d84fdf882ce9134138f141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196071b730124c9d88ea13676ea67a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6fa82bb92b4701802e236e747f41ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47706e0fc264e6a891e5a5dfbb22940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "# Train the VAD model\n",
    "vad = VoiceActivityDetection(cow_audio, duration=1, batch_size=16)\n",
    "model = PyanNet(sincnet={'stride': 10}, task=vad)\n",
    "model.to(\"cuda\")\n",
    "output_directory = os.path.join('/mnt/', 'e', 'Files', 'Acoustic_Data', 'Datasets')\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", max_epochs=5, default_root_dir=output_directory)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model for inference\n",
    "test_file = next(cow_audio.test())\n",
    "inference = Inference(model)\n",
    "vad_probability = inference(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert output to timeline and then to RTTM format\n",
    "pipeline = VoiceActivityDetectionPipeline(segmentation=model)\n",
    "initial_params = {\"onset\": 0.3, \"offset\": 0.2, \"min_duration_on\": 0.0, \"min_duration_off\": 0.0}\n",
    "pipeline.instantiate(initial_params)\n",
    "timeline = pipeline(test_file).get_timeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection error rate = 39.8%\n"
     ]
    }
   ],
   "source": [
    "metric = DetectionErrorRate()\n",
    "\n",
    "for file in cow_audio.test():\n",
    "    \n",
    "    # apply the voice activity detection pipeline\n",
    "    speech = pipeline(file)\n",
    "\n",
    "    # evaluate its output\n",
    "    _ = metric(\n",
    "        file['annotation'],     # this is the reference annotation\n",
    "        speech,                 # this is the hypothesized annotation\n",
    "        uem=file['annotated'])  # this is the part of the file that should be evaluated\n",
    "    \n",
    "# aggregate the performance over the whole test set\n",
    "detection_error_rate = abs(metric)\n",
    "print(f'Detection error rate = {detection_error_rate * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the RTTM before parameter optimization\n",
    "Get_RTTM.save_rttm(test_file, pipeline, os.path.join('/mnt/', 'e', 'Files', 'Acoustic_Data', 'Datasets', 'output_rttms','before_optimization.rttm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'onset': 0.8103251822269171, 'offset': 0.6810206947236057, 'min_duration_on': 0.21402324390136784, 'min_duration_off': 0.12266215715079959}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyannote.audio.pipelines.voice_activity_detection.VoiceActivityDetection at 0x7f1e0ac05880>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize parameters\n",
    "optimizer = Optimizer(pipeline)\n",
    "optimizer.tune(list(cow_audio.development()), \n",
    "               warm_start=initial_params, \n",
    "               n_iterations=20, \n",
    "               show_progress=False)\n",
    "optimized_params = optimizer.best_params\n",
    "print(optimized_params)\n",
    "\n",
    "pipeline.instantiate(optimized_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the RTTM after parameter optimization\n",
    "Get_RTTM.save_rttm(test_file, pipeline, os.path.join('/mnt/', 'e', 'Files', 'Acoustic_Data', 'Datasets', 'output_rttms','after_optimization.rttm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TXT_RTTM_Transform\n",
    "input_file = os.path.join('/mnt/', 'e', 'Files', 'Acoustic_Data', 'Datasets', 'output_rttms','after_optimization.rttm')\n",
    "output_file = os.path.join('/mnt/', 'e', 'Files', 'Acoustic_Data', 'Datasets', 'output_rttms','after_optimization.txt')\n",
    "TXT_RTTM_Transform.rttm_to_txt(input_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
